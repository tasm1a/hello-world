##Perceptron Algorithm
##This code will classify a few 2D points into one of two categories:
##Class 1: Above the line x + y = 1.5
##Class 0: Below the line

import pandas as pd
import random
import matplotlib.pyplot as plt
import numpy as np


# Sample dataset (4 points in 2D)
data = {
    'x1': [0.5, 1.0, 1.5, 2.0],
    'x2': [1.0, 0.5, 1.5, 1.0],
    'label': [0, 0, 1, 1]
}

df = pd.DataFrame(data)
X = df[['x1', 'x2']]
y = df['label']

# Perceptron trick: update weights if prediction is wrong
def perceptron_trick(weights, bias, x, y_true, learning_rate=0.1):
    prediction = 1 if sum(w * xi for w, xi in zip(weights, x)) + bias > 0 else 0
    error = y_true - prediction
    if error != 0:
        weights = [w + learning_rate * error * xi for w, xi in zip(weights, x)]
        bias += learning_rate * error
    return weights, bias

# Total error function
def total_error(weights, bias, X, y):
    error = 0
    for i in range(len(X)):
        x = X.loc[i]
        y_true = y[i]
        prediction = 1 if sum(w * xi for w, xi in zip(weights, x)) + bias > 0 else 0
        error += abs(y_true - prediction)
    return error

# Perceptron algorithm
def perceptron_algorithm(X, y, learning_rate=0.1, epochs=10):
    weights = [1.0 for _ in range(len(X.loc[0]))]
    bias = 0.0
    errors = []
    for epoch in range(epochs):
        errors.append(total_error(weights, bias, X, y))
        j = random.randint(0, len(X) - 1)
        weights, bias = perceptron_trick(weights, bias, X.loc[j], y[j], learning_rate)
    return weights, bias, errors

# Run it
final_weights, final_bias, training_errors = perceptron_algorithm(X, y)

# Plot data points
for i in range(len(X)):
    if y[i] == 0:
        plt.scatter(X.loc[i, 'x1'], X.loc[i, 'x2'], color='red', marker='o', label='Class 0' if i == 0 else "")
    else:
        plt.scatter(X.loc[i, 'x1'], X.loc[i, 'x2'], color='blue', marker='x', label='Class 1' if i == 2 else "")

# Plot decision boundary: w1*x + w2*y + b = 0 -> y = -(w1/w2)x - b/w2
x_vals = np.linspace(0, 2.5, 100)
if final_weights[1] != 0:
    y_vals = -(final_weights[0]/final_weights[1]) * x_vals - final_bias/final_weights[1]
    plt.plot(x_vals, y_vals, color='green', label='Decision Boundary')

plt.xlabel('x1')
plt.ylabel('x2')
plt.title('Perceptron Classification')
plt.legend()
plt.grid(True)
plt.show()

# Output results
print("Final weights:", final_weights)
print("Final bias:", final_bias)
print("Training error by epoch:", training_errors)
